{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition (Data Mining)\n",
    "\n",
    "**Emotion Recognition on Twitter**  \n",
    "**Name: Huang Yu-Lin, Student ID: 108061539**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for line in open('./data/tweets_DM.json', 'r'):\n",
    "    data.append(json.loads(line))\n",
    "    \n",
    "data_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns: hashtags, tweet_id, text\n",
    "text = []\n",
    "for source in data_df['_source']:\n",
    "    text.append([source['tweet']['tweet_id'], source['tweet']['text']])\n",
    "text = np.array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f0f',\n",
       "        \"@JZED74 While inappropriate AF, he likely wasn't kidding.  <LH>\"],\n",
       "       ['0x1c7f10', 'o m g Shut Up And Dance though #BlackMirror <LH>'],\n",
       "       ['0x1c7f11',\n",
       "        'On #twitch <LH> on the #Destinybeta #Destiny #Destiny2 #DestinytheGame #Hunter #Titan #Warlock #Destiny2theGame #Beta #Destiny2Beta #PS4'],\n",
       "       ...,\n",
       "       ['0x38fe1b',\n",
       "        \"I told myself I'd be twitter famous. twitter machine <LH>\"],\n",
       "       ['0x38fe1c', '..today was brutal  ..#Hungover'],\n",
       "       ['0x38fe1d',\n",
       "        'Love it when I sun burn my forehead!! NOT!! üò´üò±üôÑü§¶üèº\\u200d‚ôÄÔ∏è‚òÄÔ∏è #redheadproblems <LH> #ouch #burnt']],\n",
       "      dtype='<U252')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet_id\n",
    "text = text[np.argsort(text[:,0])]\n",
    "# check\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867535"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion label\n",
    "label_emo = pd.read_csv('./label/emotion.csv', header=None).to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f10', 'joy'],\n",
       "       ['0x1c7f11', 'anticipation'],\n",
       "       ['0x1c7f14', 'joy'],\n",
       "       ...,\n",
       "       ['0x38fe1a', 'surprise'],\n",
       "       ['0x38fe1c', 'disgust'],\n",
       "       ['0x38fe1d', 'sadness']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet_id\n",
    "label_emo = label_emo[np.argsort(label_emo[:,0])]\n",
    "# check\n",
    "label_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identification label\n",
    "label_id = pd.read_csv('./label/data_identification.csv', header=None).to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f0f', 'test'],\n",
       "       ['0x1c7f10', 'train'],\n",
       "       ['0x1c7f11', 'train'],\n",
       "       ...,\n",
       "       ['0x38fe1b', 'test'],\n",
       "       ['0x38fe1c', 'train'],\n",
       "       ['0x38fe1d', 'train']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet id\n",
    "label_id = label_id[np.argsort(label_id[:,0])]\n",
    "# check\n",
    "label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_idx = np.where(label_id[:,1] == 'train')\n",
    "test_idx = np.where(label_id[:,1] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 2)\n",
      "(1455563,)\n",
      "(411972, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = text[train_idx].reshape(len(train_idx[0]), -1) # hashtag, text\n",
    "test_data = text[test_idx].reshape(len(test_idx[0]), -1)\n",
    "train_label = label_emo[:,1:].reshape(-1,)\n",
    "# check shape\n",
    "assert len(train_data) == len(train_label)\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>\n",
       "1  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...\n",
       "2  0x1c7f14  A nice sunny wak this morning not many <LH> ar...\n",
       "3  0x1c7f15  I'm one of those people who love candy corn......\n",
       "4  0x1c7f16  @metmuseum What are these? They look like some..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.DataFrame(train_data, columns =['tweet_id', 'text']) \n",
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f17</td>\n",
       "      <td>Looking back on situations old &amp; new, recent o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f18</td>\n",
       "      <td>@jasoninthehouse Why do you insist on talking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...\n",
       "1  0x1c7f12  I tried to figure out why you mean so much to ...\n",
       "2  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...\n",
       "3  0x1c7f17  Looking back on situations old & new, recent o...\n",
       "4  0x1c7f18  @jasoninthehouse Why do you insist on talking ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.DataFrame(test_data, columns =['tweet_id', 'text']) \n",
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check class distribution\n",
    "distribution = {}\n",
    "for emo in train_label:\n",
    "    if emo in distribution:\n",
    "        distribution[emo] += 1\n",
    "    else:\n",
    "        distribution[emo] = 1\n",
    "emo_lst = [key for key in distribution.keys()]\n",
    "num_lst = np.array([distribution[emo] for emo in emo_lst])\n",
    "num_lst = num_lst / sum(num_lst)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.bar(emo_lst, num_lst)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can find out that it's an unbalanced dataset. (8 categories)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# encode label\n",
    "from keras import utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_label)\n",
    "train_label_encoded = label_encoder.transform(train_label)\n",
    "# label encode (one hot)\n",
    "y_train = utils.to_categorical(train_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'anticipation', 'joy', ..., 'surprise', 'disgust',\n",
       "       'sadness'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 4, ..., 6, 2, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode the label from string to numbers (Ex: 'joy' -> 4).  \n",
    "  Then, turn into one hot for classification (Ex: 4 -> 0 0 0 0 1 0 0 0).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for text preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(sentence, save_stop_words=False):\n",
    "    # remove punctuation\n",
    "    print('remove punctuation')\n",
    "    sentence = sentence.apply(lambda x: remove_punctuation(x))\n",
    "    # tokenizer\n",
    "    print('tokenize')\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    sentence = sentence.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    # remove stop words\n",
    "    if not save_stop_words:\n",
    "        print('remove stop words')\n",
    "        sentence = sentence.apply(lambda x: remove_stopwords(x))\n",
    "    # lemmatizer\n",
    "    print('lemmatize')\n",
    "    sentence = sentence.apply(lambda x: word_lemmatizer(x))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the text plays an important role in NLP.**    \n",
    "  * **Procedure: remove punctuation -> tokenize -> (remove stop words) -> lemmatize**  \n",
    "  *Since stop words might be useful in lstm model, I run two different preprocess procedure.(saving stop words or not)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation\n",
      "tokenize\n",
      "remove stop words\n",
      "lemmatize\n",
      "remove punctuation\n",
      "tokenize\n",
      "remove stop words\n",
      "lemmatize\n"
     ]
    }
   ],
   "source": [
    "# clean test data\n",
    "train_data_df['text_clean'] = preprocess(train_data_df['text'], save_stop_words=False)\n",
    "test_data_df['text_clean'] = preprocess(test_data_df['text'], save_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation\n",
      "tokenize\n",
      "lemmatize\n",
      "remove punctuation\n",
      "tokenize\n",
      "lemmatize\n"
     ]
    }
   ],
   "source": [
    "# clean test data (save stop words)\n",
    "train_data_df['text_clean_ws'] = preprocess(train_data_df['text'], save_stop_words=True)\n",
    "test_data_df['text_clean_ws'] = preprocess(test_data_df['text'], save_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.455563e+06\n",
       "mean     1.487073e+01\n",
       "std      6.299944e+00\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+01\n",
       "50%      1.500000e+01\n",
       "75%      2.000000e+01\n",
       "max      4.300000e+01\n",
       "Name: sentence_len, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence length analysis\n",
    "train_data_df['sentence_len'] = train_data_df['text_clean_ws'].apply(lambda x: len(x))\n",
    "train_data_df['sentence_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    411972.000000\n",
       "mean         17.800661\n",
       "std           5.289889\n",
       "min           3.000000\n",
       "25%          14.000000\n",
       "50%          18.000000\n",
       "75%          22.000000\n",
       "max          59.000000\n",
       "Name: sentence_len, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df['sentence_len'] = test_data_df['text_clean_ws'].apply(lambda x: len(x))\n",
    "test_data_df['sentence_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed text data\n",
    "train_data_df.to_pickle('./data/train.pkl')\n",
    "test_data_df.to_pickle('./data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_ws</th>\n",
       "      <th>sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>[g, shut, dance, though, blackmirror, lh]</td>\n",
       "      <td>[o, m, g, shut, up, and, dance, though, blackm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>[twitch, lh, destinybeta, destiny, destiny2, d...</td>\n",
       "      <td>[on, twitch, lh, on, the, destinybeta, destiny...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "      <td>[nice, sunny, wak, morning, many, lh, aroud, w...</td>\n",
       "      <td>[a, nice, sunny, wak, this, morning, not, many...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "      <td>[im, one, people, love, candy, corn, lot, üòÅüòÇ, ...</td>\n",
       "      <td>[im, one, of, those, people, who, love, candy,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "      <td>[metmuseum, look, like, something, toddler, ma...</td>\n",
       "      <td>[metmuseum, what, are, these, they, look, like...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>   \n",
       "1  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...   \n",
       "2  0x1c7f14  A nice sunny wak this morning not many <LH> ar...   \n",
       "3  0x1c7f15  I'm one of those people who love candy corn......   \n",
       "4  0x1c7f16  @metmuseum What are these? They look like some...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0          [g, shut, dance, though, blackmirror, lh]   \n",
       "1  [twitch, lh, destinybeta, destiny, destiny2, d...   \n",
       "2  [nice, sunny, wak, morning, many, lh, aroud, w...   \n",
       "3  [im, one, people, love, candy, corn, lot, üòÅüòÇ, ...   \n",
       "4  [metmuseum, look, like, something, toddler, ma...   \n",
       "\n",
       "                                       text_clean_ws  sentence_len  \n",
       "0  [o, m, g, shut, up, and, dance, though, blackm...            10  \n",
       "1  [on, twitch, lh, on, the, destinybeta, destiny...            16  \n",
       "2  [a, nice, sunny, wak, this, morning, not, many...            24  \n",
       "3  [im, one, of, those, people, who, love, candy,...            18  \n",
       "4  [metmuseum, what, are, these, they, look, like...            17  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_ws</th>\n",
       "      <th>sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "      <td>[jzed74, inappropriate, af, likely, wasnt, kid...</td>\n",
       "      <td>[jzed74, while, inappropriate, af, he, likely,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "      <td>[tried, figure, mean, much, couldnt, think, si...</td>\n",
       "      <td>[i, tried, to, figure, out, why, you, mean, so...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "      <td>[‚Äúbig, plan, ‚Äù, ever, life, promote, turnbullm...</td>\n",
       "      <td>[the, only, ‚Äúbig, plan, ‚Äù, you, ever, had, in,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f17</td>\n",
       "      <td>Looking back on situations old &amp; new, recent o...</td>\n",
       "      <td>[looking, back, situation, old, new, recent, w...</td>\n",
       "      <td>[looking, back, on, situation, old, new, recen...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f18</td>\n",
       "      <td>@jasoninthehouse Why do you insist on talking ...</td>\n",
       "      <td>[jasoninthehouse, insist, talking, clinton, wh...</td>\n",
       "      <td>[jasoninthehouse, why, do, you, insist, on, ta...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...   \n",
       "1  0x1c7f12  I tried to figure out why you mean so much to ...   \n",
       "2  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...   \n",
       "3  0x1c7f17  Looking back on situations old & new, recent o...   \n",
       "4  0x1c7f18  @jasoninthehouse Why do you insist on talking ...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [jzed74, inappropriate, af, likely, wasnt, kid...   \n",
       "1  [tried, figure, mean, much, couldnt, think, si...   \n",
       "2  [‚Äúbig, plan, ‚Äù, ever, life, promote, turnbullm...   \n",
       "3  [looking, back, situation, old, new, recent, w...   \n",
       "4  [jasoninthehouse, insist, talking, clinton, wh...   \n",
       "\n",
       "                                       text_clean_ws  sentence_len  \n",
       "0  [jzed74, while, inappropriate, af, he, likely,...             9  \n",
       "1  [i, tried, to, figure, out, why, you, mean, so...            28  \n",
       "2  [the, only, ‚Äúbig, plan, ‚Äù, you, ever, had, in,...            19  \n",
       "3  [looking, back, on, situation, old, new, recen...            22  \n",
       "4  [jasoninthehouse, why, do, you, insist, on, ta...            24  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed text data\n",
    "train_data_df = pd.read_pickle('./data/train.pkl')\n",
    "test_data_df = pd.read_pickle('./data/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def get_seqs(text, tokenizer, max_length=128):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "def train_tokenizer(X, max_num_words=20000):\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    return tokenizer\n",
    "\n",
    "def build_weight(y):\n",
    "    y_int = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_int), y_int)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights\n",
    "\n",
    "def train_model(model, X, y, setup):\n",
    "    class_weight = build_weight(y)\n",
    "    callbacks = [ModelCheckpoint(filepath='model_{}.h5'.format(setup['type']), \n",
    "                                 monitor='val_accuracy', \n",
    "                                 save_best_only=True)]\n",
    "    optimizer = optimizers.Adam(lr=setup['lr'], clipvalue=1.0)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(X, y, epochs=setup['epochs'], batch_size=setup['batch_size'], callbacks=callbacks,\n",
    "              validation_split=0.1, class_weight=class_weight)\n",
    "    return model\n",
    "\n",
    "def show_result(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('mean f1 score: %.4f' % f1)\n",
    "    print('confusion matrix:\\n{}'.format(cm))\n",
    "    return f1\n",
    "\n",
    "def make_prediction(model, X_train, y_train, X_test, setup):\n",
    "    # train model\n",
    "    model = train_model(model, X_train, y_train, setup)\n",
    "    # predict\n",
    "    model.load_weights('model_{}.h5'.format(setup['type']))\n",
    "    probs = model.predict(X_test, verbose=1)\n",
    "    preds = np.argmax(probs, axis=1).reshape(-1,)\n",
    "    # record\n",
    "    return preds\n",
    "\n",
    "def make_submission(tweet_id_lst, emotion_lst, model_type):\n",
    "    assert len(tweet_id_lst) == len(emotion_lst)\n",
    "    prediction_dict = {}\n",
    "    for idx, tweet_id in enumerate(tweet_id_lst):\n",
    "        prediction_dict[tweet_id] = emotion_lst[idx]\n",
    "    # submission\n",
    "    submission_df = pd.read_csv('./sampleSubmission.csv')\n",
    "    # write\n",
    "    assert len(tweet_id_lst) == len(submission_df['id'])\n",
    "    for idx, tweet_id in enumerate(submission_df['id']):\n",
    "        submission_df.iloc[idx]['emotion'] = prediction_dict[tweet_id]\n",
    "    submission_df.to_csv(\"./submission_{}.csv\".format(model_type), index=False)\n",
    "    print('submission file is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation to evaluate the method\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_evaluate(X, y, setup, cv=5):\n",
    "    # train\n",
    "    kf = KFold(n_splits=cv)\n",
    "    y_pred_lst, y_true_lst = [], []\n",
    "    for idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print('Fold - {}'.format(idx+1))\n",
    "        # split\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        # build model\n",
    "        if setup['type'] == 'dnn':\n",
    "            model = dnn(setup)\n",
    "        elif setup['type'] == 'blstm_att':\n",
    "            model = blstm_att(setup)\n",
    "        else:\n",
    "            raise Exception('Undefined structure')\n",
    "        # train model & make prediction\n",
    "        preds = make_prediction(model, X_train, y_train, X_test, setup)\n",
    "        # record\n",
    "        y_true = np.argmax(y_val, axis=1).reshape(-1,)\n",
    "        y_true_lst.extend(y_true)\n",
    "        y_pred_lst.extend(preds)\n",
    "    # evaluate\n",
    "    show_result(y_true_lst, y_pred_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To select the best model with best parameters setting, cross validation is a great method to evaluate the performance of different methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. \n",
    "tfidf + dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "def dnn(setup):\n",
    "    # parameter setting\n",
    "    input_dim = setup['max_features']\n",
    "    p = setup['dropout']\n",
    "    weight_decay = setup['weight_decay']\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay),\n",
    "                    input_shape=(input_dim,)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    model.add(Dense(64, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    model.add(Dense(8, activation='softmax',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate by cross validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# parameter setup\n",
    "setup = {'max_features': 20000, 'dropout': 0.5, 'weight_decay': 1e-4, \n",
    "         'batch_size': 256, 'lr': 0.001, 'epochs': 10, 'type': 'dnn'}\n",
    "\n",
    "# train vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=setup['weight_decay'])\n",
    "vectorizer.fit(train_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# feature encode\n",
    "X_train = vectorizer.transform(train_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# cross validation\n",
    "cv_evaluate(X_train, y_train, setup, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on whole dataset and make submission\n",
    "\n",
    "# feature encode\n",
    "X_test = vectorizer.transform(test_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# train & predict\n",
    "model = dnn(setup)\n",
    "y_pred = make_prediction(model, X_train, y_train, X_test, setup)\n",
    "\n",
    "# make submission\n",
    "emotion_lst = label_encoder.inverse_transform(y_pred)\n",
    "make_submission(test_data_df['tweet_id'], emotion_lst, setup['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2. \n",
    "word2vec + blstm + self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention layer\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, att_head, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.att_head = att_head\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.vec_dim = int(self.output_dim*self.att_head)\n",
    "        self.pro_kernel = self.add_weight(name='kernel_p',\n",
    "                                        shape=(3, input_shape[2], self.vec_dim),\n",
    "                                        initializer='orthogonal',\n",
    "                                        trainable=True)\n",
    "        self.out_kernel = self.add_weight(name='kernal_o',\n",
    "                                        shape=(1, self.vec_dim, input_shape[2]),\n",
    "                                        initializer='orthogonal',\n",
    "                                        trainable=True)\n",
    "        super(Self_Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # projection\n",
    "        WQ = K.dot(x, self.pro_kernel[0])\n",
    "        WK = K.dot(x, self.pro_kernel[1])\n",
    "        WV = K.dot(x, self.pro_kernel[2])\n",
    "        # attention score\n",
    "        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))\n",
    "        # scale\n",
    "        QK = QK / (self.vec_dim**0.5)\n",
    "        # softmax normalization\n",
    "        QK = K.softmax(QK)\n",
    "        # weighted vector \n",
    "        V = K.batch_dot(QK, WV)\n",
    "        # project to output space\n",
    "        Z = K.dot(V, self.out_kernel[0])\n",
    "        return Z\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "# define structure\n",
    "def blstm_att(setup):\n",
    "    # parameter setting\n",
    "    max_num_words = setup['max_num_words']\n",
    "    max_length = setup['max_length']\n",
    "    att_head = setup['att_head']\n",
    "    p = setup['dropout']\n",
    "    weight_decay = setup['weight_decay']\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # word embedding\n",
    "    model.add(Embedding(max_num_words, 128, input_length=max_length))\n",
    "    # blstm\n",
    "    model.add(Bidirectional(LSTM(units=128, \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=p, \n",
    "                                 recurrent_dropout=p, \n",
    "                                 kernel_initializer='orthogonal',\n",
    "                                 kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                                 bias_regularizer=keras.regularizers.l2(weight_decay)\n",
    "                                 )))\n",
    "    # self attention\n",
    "    model.add(Self_Attention(128, att_head))\n",
    "    # average pooling\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=p))\n",
    "    # dimension reduction\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    # classifier\n",
    "    model.add(Dense(8, activation='softmax',\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate by cross validation\n",
    "\n",
    "# parameter setup\n",
    "setup = {'max_num_words': 100000, 'max_length': 64, 'att_head': 4, 'dropout': 0.5, 'weight_decay': 1e-4,\n",
    "         'batch_size': 128, 'lr': 0.001, 'epochs': 5, 'type': 'blstm'}\n",
    "\n",
    "# train tokenizer\n",
    "tokenizer = train_tokenizer(train_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), max_num_words)\n",
    "\n",
    "# feature encode\n",
    "X_train = get_seqs(train_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), tokenizer, max_length)\n",
    "\n",
    "# cross validation\n",
    "cv_evaluate(X_train, y_train, setup, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on whole dataset and make submission\n",
    "\n",
    "# feature encode\n",
    "X_test = get_seqs(test_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), tokenizer, max_length)\n",
    "\n",
    "# train & predict\n",
    "model = blstm_att(setup)\n",
    "y_pred = make_prediction(model, X_train, y_train, X_test, setup)\n",
    "\n",
    "# make submission\n",
    "emotion_lst = label_encoder.inverse_transform(y_pred)\n",
    "make_submission(test_data_df['tweet_id'], emotion_lst, setup['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
