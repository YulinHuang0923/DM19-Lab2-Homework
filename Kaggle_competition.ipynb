{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition (Data Mining)\n",
    "\n",
    "**Emotion Recognition on Twitter**  \n",
    "**Name: Huang Yu-Lin, Student ID: 108061539**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for line in open('./data/tweets_DM.json', 'r'):\n",
    "    data.append(json.loads(line))\n",
    "    \n",
    "data_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns: hashtags, tweet_id, text\n",
    "text = []\n",
    "for source in data_df['_source']:\n",
    "    text.append([source['tweet']['tweet_id'], source['tweet']['text']])\n",
    "text = np.array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f0f',\n",
       "        \"@JZED74 While inappropriate AF, he likely wasn't kidding.  <LH>\"],\n",
       "       ['0x1c7f10', 'o m g Shut Up And Dance though #BlackMirror <LH>'],\n",
       "       ['0x1c7f11',\n",
       "        'On #twitch <LH> on the #Destinybeta #Destiny #Destiny2 #DestinytheGame #Hunter #Titan #Warlock #Destiny2theGame #Beta #Destiny2Beta #PS4'],\n",
       "       ...,\n",
       "       ['0x38fe1b',\n",
       "        \"I told myself I'd be twitter famous. twitter machine <LH>\"],\n",
       "       ['0x38fe1c', '..today was brutal  ..#Hungover'],\n",
       "       ['0x38fe1d',\n",
       "        'Love it when I sun burn my forehead!! NOT!! üò´üò±üôÑü§¶üèº\\u200d‚ôÄÔ∏è‚òÄÔ∏è #redheadproblems <LH> #ouch #burnt']],\n",
       "      dtype='<U252')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet_id\n",
    "text = text[np.argsort(text[:,0])]\n",
    "# check\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867535"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion label\n",
    "label_emo = pd.read_csv('./label/emotion.csv', header=None).to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f10', 'joy'],\n",
       "       ['0x1c7f11', 'anticipation'],\n",
       "       ['0x1c7f14', 'joy'],\n",
       "       ...,\n",
       "       ['0x38fe1a', 'surprise'],\n",
       "       ['0x38fe1c', 'disgust'],\n",
       "       ['0x38fe1d', 'sadness']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet_id\n",
    "label_emo = label_emo[np.argsort(label_emo[:,0])]\n",
    "# check\n",
    "label_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455563"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identification label\n",
    "label_id = pd.read_csv('./label/data_identification.csv', header=None).to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0x1c7f0f', 'test'],\n",
       "       ['0x1c7f10', 'train'],\n",
       "       ['0x1c7f11', 'train'],\n",
       "       ...,\n",
       "       ['0x38fe1b', 'test'],\n",
       "       ['0x38fe1c', 'train'],\n",
       "       ['0x38fe1d', 'train']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by tweet id\n",
    "label_id = label_id[np.argsort(label_id[:,0])]\n",
    "# check\n",
    "label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_idx = np.where(label_id[:,1] == 'train')\n",
    "test_idx = np.where(label_id[:,1] == 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 2)\n",
      "(1455563,)\n",
      "(411972, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = text[train_idx].reshape(len(train_idx[0]), -1) # hashtag, text\n",
    "test_data = text[test_idx].reshape(len(test_idx[0]), -1)\n",
    "train_label = label_emo[:,1:].reshape(-1,)\n",
    "# check shape\n",
    "assert len(train_data) == len(train_label)\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>\n",
       "1  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...\n",
       "2  0x1c7f14  A nice sunny wak this morning not many <LH> ar...\n",
       "3  0x1c7f15  I'm one of those people who love candy corn......\n",
       "4  0x1c7f16  @metmuseum What are these? They look like some..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.DataFrame(train_data, columns =['tweet_id', 'text']) \n",
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f17</td>\n",
       "      <td>Looking back on situations old &amp; new, recent o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f18</td>\n",
       "      <td>@jasoninthehouse Why do you insist on talking ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...\n",
       "1  0x1c7f12  I tried to figure out why you mean so much to ...\n",
       "2  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...\n",
       "3  0x1c7f17  Looking back on situations old & new, recent o...\n",
       "4  0x1c7f18  @jasoninthehouse Why do you insist on talking ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.DataFrame(test_data, columns =['tweet_id', 'text']) \n",
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAEWCAYAAACKZoWNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZgdZZn38e/PIJtAWNI6koUOENQwjixNEBUERAgiBBUlKA6or7mY14jKOGN4cSDGQVkcFzQO4BBQFCOLYoRoQBCQTdNACATI0IRAYlgCYZUQ6OR+/6intXI4p7s6faq7U/l9rquvrnrqqaq76tQ593mqnlOliMDMzMyq5XUDHYCZmZk1nxO8mZlZBTnBm5mZVZATvJmZWQU5wZuZmVWQE7yZmVkFOcGbVYykcyX9Rz+sZ39JS3PjCyTt36Rlf0LSNbnxkLRzM5adlveipB2btTyzwUj+HbxZ80laDLwJWJ0rvigiJjd5PccD/yci3tPM5RZc9/7ATyNiRC/maQUeBl4fEZ29mC+AMRHR0cswkXQDWZz/09t5zdZnGw10AGYVdnhE/H6gg1jfSNqoN8nfzOrzKXqzfibpeEm3SPqOpGclLZL0rlS+RNKTko7L1R8q6SeSlkt6RNJXJb1O0tuAc4F90innZ1P9iyT9Z27+z0rqkLRC0ixJ2+emhaQTJD0o6RlJ0yWpQdybpWU/I+k+YK+a6YslHZSGx0lql/S8pCckfTtVuyn9fzbFvE/N/lgBTE1lN9eE8IG0r56SdLak16V1TZX001wcrWm7NpJ0OrAv8IO0vh/ktnvn7vZv7rW6WdK30nY/LOnQQi+02QBzgjcbGHsD84HtgEuAmWQJc2fgWLKEtEWq+31gKLAj8F7gn4FPRcT9wAnAbRGxRURsXbsSSQcC3wQ+BrwZeCStK++Dad3vSPUOaRDzacBO6e8Q4LgG9QC+B3wvIrZK9S9N5ful/1unmG/L7Y9FwBuB0xss80NAG7AHMAH4dDfrByAiTgH+CExO66t3iaTu/s1N3xtYCAwDzgIuaPQlyGwwcYI3K8+VqYXe9ffZ3LSHI+LCiFgN/AIYCUyLiFURcQ3wCrCzpCHA0cDJEfFCRCwG/gv4ZMEYPgHMiIg7I2IVcDJZi781V+eMiHg2Ih4F/gDs1mBZHwNOj4gVEbEEOKeb9b6a4h8WES9GxO09xLksIr4fEZ0RsbJBnTPTuh8Fvgsc08Mye1Rw/z4SET9Kr9WPyb4ovamv6zYrmxO8WXmOjIitc38/yk17Ije8EiAiasu2IGs1bkzW8u7yCDC8YAzb5+eNiBeBp2vmfzw3/FJab6NlLamJo5HPALsAD0iaK+mDPcS5pIfptXUeSfH0VZH9+7f9ExEvpcFG+8hs0HCCNxvcniJrDe+QKxsF/CUN9/QzmGX5eSW9geyywF8aztHYY2RnGvJx1BURD0bEMWSn3M8ELk/rbhRvkZ/z1K57WRr+K7B5bto/9GLZPe1fs/WWE7zZIJZOC18KnC5pS0k7ACcBXZ3KngBGSNq4wSIuAT4laTdJmwDfAP6UTkX31qXAyZK2kTQC+HyjipKOldQSEWuAZ1PxamA5sIbsendv/Vta90jgC2SXNgDmAftJGiVpKNlliLwnGq2vwP41W285wZuV5zep53bX36/WcTmfJ2ulLgJuJkvaM9K064EFwOOSnqqdMSKuA/4DuIKsBb4TMHEd4/ga2enrh4FrgIu7qTseWCDpRbIOdxMj4uV0ivt04JbUL+GdvVj/r4E7yBL61cAFABFxLVmyn5+mX1Uz3/eAo1Iv+Hr9Brrbv2brLd/oxszMrILcgjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCqrMw2aGDRsWra2tAx2GmZlZv7njjjueioiWetMqk+BbW1tpb28f6DDMzMz6jaSGd5T0KXozM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOroFITvKTxkhZK6pA0pc70EyTdI2mepJsljU3lrZJWpvJ5ks4tM04zM7OqKe1OdpKGANOB9wNLgbmSZkXEfblql0TEuan+EcC3gfFp2kMRsVtZ8RXROuXqgVx9IYvPOGygQzAzs0GozBb8OKAjIhZFxCvATGBCvkJEPJ8bfQMQJcZjZma2wSgzwQ8HluTGl6aytUj6nKSHgLOAE3OTRku6S9KNkvattwJJkyS1S2pfvnx5M2M3MzNbr5WZ4FWn7DUt9IiYHhE7AV8BvpqKHwNGRcTuwEnAJZK2qjPv+RHRFhFtLS11H6ZjZma2QSozwS8FRubGRwDLuqk/EzgSICJWRcTTafgO4CFgl5LiNDMzq5wyE/xcYIyk0ZI2BiYCs/IVJI3JjR4GPJjKW1InPSTtCIwBFpUYq5mZWaWU1os+IjolTQbmAEOAGRGxQNI0oD0iZgGTJR0EvAo8AxyXZt8PmCapE1gNnBARK8qK1czMrGpKS/AAETEbmF1Tdmpu+AsN5rsCuKLM2MzMzKrMd7IzMzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCio1wUsaL2mhpA5JU+pMP0HSPZLmSbpZ0tjctJPTfAslHVJmnGZmZlVTWoKXNASYDhwKjAWOySfw5JKIeHtE7AacBXw7zTsWmAjsCowHfpiWZ2ZmZgWU2YIfB3RExKKIeAWYCUzIV4iI53OjbwAiDU8AZkbEqoh4GOhIyzMzM7MCNipx2cOBJbnxpcDetZUkfQ44CdgYODA37+018w4vJ0wzM7PqKbMFrzpl8ZqCiOkRsRPwFeCrvZlX0iRJ7ZLaly9f3qdgzczMqqTMBL8UGJkbHwEs66b+TODI3swbEedHRFtEtLW0tPQxXDMzs+ooM8HPBcZIGi1pY7JOc7PyFSSNyY0eBjyYhmcBEyVtImk0MAb4c4mxmpmZVUpp1+AjolPSZGAOMASYERELJE0D2iNiFjBZ0kHAq8AzwHFp3gWSLgXuAzqBz0XE6rJiNTMzq5oyO9kREbOB2TVlp+aGv9DNvKcDp5cXnZmZWXX5TnZmZmYV5ARvZmZWQU7wZmZmFeQEb2ZmVkFO8GZmZhXkBG9mZlZBTvBmZmYV5ARvZmZWQU7wZmZmFeQEb2ZmVkFO8GZmZhXkBG9mZlZBTvBmZmYV5ARvZmZWQU7wZmZmFeQEb2ZmVkFO8GZmZhXkBG9mZlZBTvBmZmYV5ARvZmZWQU7wZmZmFVRqgpc0XtJCSR2SptSZfpKk+yTNl3SdpB1y01ZLmpf+ZpUZp5mZWdVsVNaCJQ0BpgPvB5YCcyXNioj7ctXuAtoi4iVJ/wKcBRydpq2MiN3Kis/MzKzKymzBjwM6ImJRRLwCzAQm5CtExB8i4qU0ejswosR4zMzMNhhlJvjhwJLc+NJU1shngN/mxjeV1C7pdklH1ptB0qRUp3358uV9j9jMzKwiSjtFD6hOWdStKB0LtAHvzRWPiohlknYErpd0T0Q8tNbCIs4Hzgdoa2uru2wzM7MNUY8teEk7SdokDe8v6URJWxdY9lJgZG58BLCszvIPAk4BjoiIVV3lEbEs/V8E3ADsXmCdZmZmRrFT9FcAqyXtDFwAjAYuKTDfXGCMpNGSNgYmAmv1hpe0O3AeWXJ/Mle+Te5LxTDg3UC+c56ZmZl1o8gp+jUR0SnpQ8B3I+L7ku7qaaY0z2RgDjAEmBERCyRNA9ojYhZwNrAFcJkkgEcj4gjgbcB5ktaQfQk5o6b3vZmZmXWjSIJ/VdIxwHHA4ans9UUWHhGzgdk1Zafmhg9qMN+twNuLrMPMzMxeq8gp+k8B+wCnR8TDkkYDPy03LDMzM+uLHlvwEXGfpK8Ao9L4w8AZZQdmZmZm665IL/rDgXnA79L4br51rJmZ2eBW5BT9VLK70j0LEBHzyHrSm5mZ2SBVJMF3RsRzNWW+qYyZmdkgVqQX/b2SPg4MkTQGOBG4tdywzMzMrC+KtOA/D+wKrCK7wc1zwBfLDMrMzMz6pkgv+pfIbiV7SvnhmJmZWTMU6UV/bf7e8+k2snPKDcvMzMz6osgp+mER8WzXSEQ8A7yxvJDMzMysr4ok+DWSRnWNSNoB96I3MzMb1Ir0oj8FuFnSjWl8P2BSeSGZmZlZXxXpZPc7SXsA7wQEfCkinio9MjMzM1tnRVrwAJsAK1L9sZKIiJvKC8vMzMz6oscEL+lM4GhgAbAmFQfgBG9mZjZIFWnBHwm8JSJWlR2MmZmZNUeRXvSLgNeXHYiZmZk1T5EW/EvAPEnXkd2uFoCIOLG0qMzMzKxPiiT4WenPzMzM1hNFfib34/4IxMzMzJqnyL3ox0i6XNJ9khZ1/RVZuKTxkhZK6pA0pc70k9Jy50u6Lt0lr2vacZIeTH/H9W6zzMzMNmxFOtldCPw30AkcAPwEuLinmSQNAaYDhwJjgWMkja2pdhfQFhH/BFwOnJXm3RY4DdgbGAecJmmbIhtkZmZmxRL8ZhFxHaCIeCQipgIHFphvHNAREYsi4hVgJjAhXyEi/pAeRwtwOzAiDR8CXBsRK9LDba4FxhdYp5mZmVGsk93Lkl4HPChpMvAXij1NbjiwJDe+lKxF3shngN92M+/w2hkkTSLdF3/UqFG1k83MzDZYRVrwXwQ2B04E9gSOBf65wHyqU1b3KXSSjgXagLN7M29EnB8RbRHR1tLSUiAkMzOzDUORBN8aES9GxNKI+FREfAQo0lxeCozMjY8AltVWknQQ2RPrjsjdLa/QvGZmZlZfkQR/csGyWnOBMZJGS9oYmEjN7+kl7Q6cR5bcn8xNmgMcLGmb1Lnu4FRmZmZmBTS8Bi/pUOADwHBJ5+QmbUXWo75bEdGZrtnPAYYAMyJigaRpQHtEzCI7Jb8FcJkkgEcj4oiIWCHp62RfEgCmRcSKddg+MzOzDVJ3neyWAe3AEcAdufIXgC8VWXhEzAZm15Sdmhs+qJt5ZwAziqzHzMzM1tYwwUfE3cDdki6JiFcB0unykemna2ZmZjZIFbkGf62krdLNZ+4GLpT07ZLjMjMzsz4okuCHRsTzwIeBCyNiT6DhqXUzMzMbeEUS/EaS3gx8DLiq5HjMzMysCYok+GlkPeE7ImKupB2BB8sNy8zMzPqiyONiLwMuy40vAj5SZlBmZmbWNz0meEktwGeB1nz9iPh0eWGZmZlZXxR52MyvgT8CvwdWlxuOmZmZNUORBL95RHyl9EjMzMysaYp0srtK0gdKj8TMzMyapkiC/wJZkl8p6XlJL0h6vuzAzMzMbN0V6UW/ZX8EYuVrnXL1QIfQo8VnHDbQIZiZVUJ3T5N7a0Q8IGmPetMj4s7ywjIzM7O+6K4FfxIwCfivOtMCOLCUiMzMzKzPunua3KT0/4D+C8fMzMyaoUgnOzMzM1vPOMGbmZlVUMMEL+nd6f8m/ReOmZmZNUN3Lfhz0v/b+iMQMzMza57uetG/KulCYLikc2onRsSJ5YVlZmZmfdFdC/6DZM+Bfxm4o85fjySNl7RQUoekKXWm7yfpTkmdko6qmbZa0rz0N6voBpmZmVn3P5N7Cpgp6f6IuLu3C5Y0BJgOvB9YCsyVNCsi7stVexQ4HvhynUWsjIjderteMzMzK9aL/mlJv5L0pKQnJF0haUSB+cYBHRGxKCJeAWYCE/IVImJxRMwH1vQ+dDMzM2ukSIK/EJgFbA8MB36TynoyHFiSG1+ayoraVFK7pNslHVmvgqRJqU778uXLe7FoMzOzaiuS4N8YERdGRGf6uwhoKTCf6pRFL2IbFRFtwMeB70ra6TULizg/Itoioq2lpUhIZmZmG4YiCX65pGMlDUl/xwJPF5hvKTAyNz4CWFY0sIhYlv4vAm4Adi86r5mZ2YauSIL/NPAx4HHgMeCoVNaTucAYSaMlbQxMJDvV3yNJ23TdYEfSMODdwH3dz2VmZmZdijwP/lHgiN4uOCI6JU0m+6ndEGBGRCyQNA1oj4hZkvYCfgVsAxwu6WsRsSvwNuA8SWvIvoScUdP73szMzLrRY4Lvi4iYDcyuKTs1NzyX7NR97Xy3Am8vMzazwaZ1ytUDHUKPFp9x2ECHYGYF+WEzZmZmFeQEb2ZmVkGFE7ykd0q6XtItjX6XbmZmZoNDw2vwkv4hIh7PFZ1E1tlOwK3AlSXHZmZmZuuou05250q6Azg7Il4GniW76cwa4Pn+CM7M1l/uNGg2sBqeoo+II4F5wFWSPgl8kSy5bw74FL2Zmdkg1u01+Ij4DXAIsDXwS2BhRJwTEb7xu5mZ2SDWMMFLOkLSzcD1wL1kd6L7kKSf17svvJmZmQ0e3V2D/09gH2AzYHZEjANOkjQGOJ0s4ZuZmdkg1F2Cf44siW8GPNlVGBEP4uRuZmY2qHV3Df5DZB3qOsl6z5uZmdl6omELPiKeAr7fj7GYmZlZk/hWtWZmZhXkBG9mZlZBTvBmZmYV5ARvZmZWQU7wZmZmFeQEb2ZmVkFO8GZmZhXkBG9mZlZBpSZ4SeMlLZTUIWlKnen7SbpTUqeko2qmHSfpwfR3XJlxmpmZVU1pCV7SEGA6cCgwFjhG0tiaao8CxwOX1My7LXAasDcwDjhN0jZlxWpmZlY1ZbbgxwEdEbEoIl4BZgIT8hUiYnFEzAfW1Mx7CHBtRKyIiGeAa4HxJcZqZmZWKWUm+OHAktz40lTWtHklTZLULql9+fLl6xyomZlZ1ZSZ4FWnLJo5b0ScHxFtEdHW0tLSq+DMzMyqrMwEvxQYmRsfASzrh3nNzMw2eA0fF9sEc4ExkkYDfwEmUvy58nOAb+Q61h0MnNz8EG191jrl6oEOoUeLzzhsoEMwsw1UaS34iOgEJpMl6/uBSyNigaRpko4AkLSXpKXAR4HzJC1I864Avk72JWEuMC2VmZmZWQFltuCJiNnA7JqyU3PDc8lOv9ebdwYwo8z4zMzMqsp3sjMzM6sgJ3gzM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKcoI3MzOrICd4MzOzCnKCNzMzqyAneDMzswpygjczM6sgJ3gzM7MKKjXBSxovaaGkDklT6kzfRNIv0vQ/SWpN5a2SVkqal/7OLTNOMzOzqtmorAVLGgJMB94PLAXmSpoVEfflqn0GeCYidpY0ETgTODpNeygidisrPjMzsyorswU/DuiIiEUR8QowE5hQU2cC8OM0fDnwPkkqMSYzM7MNQpkJfjiwJDe+NJXVrRMRncBzwHZp2mhJd0m6UdK+JcZpZmZWOaWdogfqtcSjYJ3HgFER8bSkPYErJe0aEc+vNbM0CZgEMGrUqCaEbGZmVg1ltuCXAiNz4yOAZY3qSNoIGAqsiIhVEfE0QETcATwE7FK7gog4PyLaIqKtpaWlhE0wMzNbP5WZ4OcCYySNlrQxMBGYVVNnFnBcGj4KuD4iQlJL6qSHpB2BMcCiEmM1MzOrlNJO0UdEp6TJwBxgCDAjIhZImga0R8Qs4ALgYkkdwAqyLwEA+wHTJHUCq4ETImJFWbGamfWkdcrVAx1CIYvPOGygQ7BBosxr8ETEbGB2TdmpueGXgY/Wme8K4IoyYzMzM6sy38nOzMysgpzgzczMKsgJ3szMrIKc4M3MzCqo1E52ZmZm/cG/cngtt+DNzMwqyAnezMysgpzgzczMKsjX4M3MNkC+Zl19bsGbmZlVkBO8mZlZBTnBm5mZVZATvJmZWQU5wZuZmVWQE7yZmVkFOcGbmZlVkBO8mZlZBTnBm5mZVZATvJmZWQU5wZuZmVWQE7yZmVkFlZrgJY2XtFBSh6QpdaZvIukXafqfJLXmpp2cyhdKOqTMOM3MzKqmtAQvaQgwHTgUGAscI2lsTbXPAM9ExM7Ad4Az07xjgYnArsB44IdpeWZmZlZAmS34cUBHRCyKiFeAmcCEmjoTgB+n4cuB90lSKp8ZEasi4mGgIy3PzMzMCijzefDDgSW58aXA3o3qRESnpOeA7VL57TXzDq9dgaRJwKQ0+qKkhc0JvVTDgKeatTCd2awlrTNvTze8PU3n7enBAG+Tt6cHJWzPDo0mlJngVacsCtYpMi8RcT5wfu9DGziS2iOibaDjaBZvz+Dm7RncvD2D2/q+PWWeol8KjMyNjwCWNaojaSNgKLCi4LxmZmbWQJkJfi4wRtJoSRuTdZqbVVNnFnBcGj4KuD4iIpVPTL3sRwNjgD+XGKuZmVmllHaKPl1TnwzMAYYAMyJigaRpQHtEzAIuAC6W1EHWcp+Y5l0g6VLgPqAT+FxErC4r1n62Xl1SKMDbM7h5ewY3b8/gtl5vj7IGs5mZmVWJ72RnZmZWQU7wZmZmFeQEXxJJtw50DD2RdGT+7oKSpkk6qJv6bZLOWcd1bS3p/+bGt5d0+bosq+D6pkr6ck/b1MT1rbUvS1zPWvuxj8vaX9K7mrGsZpPUKunegY6jLySdKOl+ST8b6FisuSTNlrT1QMfRE1+D34BJugi4KiJKS7S5dbWmdf1j2etK65sKvBgR3+qn9V1EP+zLRvtR0pDedkTt733UG/19vJRB0gPAoelunOu6jF6/ruubdPdSRcSaAYxho4joLFBvwGPtlYjwXwl/wItkN+w5G7gXuAc4Ok27GJiQq/sz4IgmrfdK4A5gATApF8vpwN1kdwh8E/Ausl8uPAzMA3YCLgKOSvPsBdya5vkzsCWwP9mHLsDUtB3XAw8Cn03lWwDXAXembZ6QymcCK9O6zgZagXvTtE2BC1P9u4ADUvnxwC+B36V1nNXDtp8CLAR+D/wc+HLNNp1B9suM+cC3UtlOaZ/MBaaRJTzy25rGfwAcX2859fZlicdVfj/OBf4AXJLi+ds+TXW/DExNwyfmYp6Z6j4O/CUta9+S4n0DcHU6ju4FjgZOTbHfS9ZLuauhsWeqd1s6RrqOj4bHAXBwqn8ncBmwRTev9UfTOu8Gbir5/X8u8Eo6pk8BZqRtvou/vydagT+m2O8E3pU79v72upYZZzM+S7p7H6Vp/5bK5wNfy237/cAP0z7ZocTjbTEwLE1vA25Iw1PT8XdN2tfHA79Ox9lC4LRGsXYts976csfyjWn/zQHePCCv4UAdPFX/S2+EjwDXkv1M8E3Ao8CbgfcCV6Z6Q8kSw0ZNWu+26f9m6YDbjuwugIen8rOAr6bhi0jJLz8ObAwsAvZK5VuR/aRyf9ZO8Hen9Qwju+Xw9qneVqnOMLLnCIjXJp+/jQP/ClyYht+a9tOm6Q23KO2jTYFHgJENtntPsg/TzVO8HeQSPLBtetN2JZOt0/+rgGPS8An0kOC7Wc5a+7LE4yq/3/YH/gqMrp2WxvMJfhmwSU3MU4EvlxzvR4Af5caHko7RNH5x7ticD7w3Ddcm+NccB+n4ugl4Q6r3FbIvD41eo3uA4fmykrd9cYrxG8CxXesF/pcsMWwObJrKx5D9fPg1r+tA/dG7z5JG76ODSV/iyC4JXwXsl47VNcA7++F4W0zjBH8HsFnuOHssbWfXNrfVizX32tZb3+vJGkctqexosp+J9/tr6Gvw5XoP8POIWB0RT5B9o9srIm4Edpb0RuAY4IoocHqooBMldX27Hkn2wfEK2RsLsgO6tYdlvAV4LCLmAkTE8w3i+3VErIyIp8haHOPI3sjfkDSfrCU9nOzLTXfeQ/ZBT0Q8QPYBvkuadl1EPBcRL5O1yBrdd3lf4FcR8VJEPM9rb6r0PPAy8D+SPgy8lMr3IWv5QfYtvieNljNQ/hzFTgHPB34m6Viye0v0l3uAgySdKWnfiHgOOCA9Hvoe4EBgV0lDyZLujWm+i2uWU+84eCfZkypvkTSP7KZZO9D4NboFuEjSZ8m+dPeXg4EpKcYbyL6kjCJLBD9K++GytC1dir6uZerNZ0mj99HB6e8usrMUb03LAXgkIvLPHGmGesdbd2ZFxMrc+LUR8XQq+yXZZ1N3sdZb31uAfwSuTa/5V8nuxtrvyrwXvdW/p36Xi4FPkN3c59NNWZm0P3AQsE9EvCTpBrIPk1cjfZUEVtPz6y7q3Pu/jto6QbZNLcCeEfGqpMUphp7W18iq3HBPsTeMObIbL40D3ke2zyeTJZdGOlm7E+qm67icsv01N1w35uQwspbTEcB/SNq1H2IjIv5X0p7AB4BvSroG+BzQFhFLUj+ATen5mKt3HIjsA/mY2sr1XqOIOEHS3mT7Yp6k3SLi6T5vZM8EfCQi1noYVtr2J4B3kL1uL+cm51/Xftfkz5JvRsR5NctvpYRtbHC85d8XtZ9FtTHU+0yrV6+79f0KWBAR+6zjZjSNW/Dlugk4WtIQSS1kH7Bdt9y9CPgiZHfua9L6hgLPpDfkW8laON15gezaeq0HgO0l7QUgacv0rIBaEyRtKmk7stOKc1MMT6bkfgB/b3E3Whdk++kTaV27kLVuevtkwJuAD0naTNKWwOH5iZK2AIZGxGyy/b5bmnQ72Wk2SHdSTB4BxqbbJQ8lSxbdLae77Wum7tbzBPBGSdtJ2gT4IICk15Fd2vgD8O9kp4m36GFZTSFpe+CliPgpWX+FPdKkp9K+PAogIp4FnpPU1WL6RIHF3w68W9LOaV2bS9ql0WskaaeI+FNEnEr2hLCRjRbcZHOAz6cOWkjaPZUPJTtTtgb4JP17VqEnvf0safQ+mgN8Or0mSBqezlyWosHxtpjsEh65GBt5v6RtJW0GHEl21qe361sItEjaJ9V5fX99oa7lFnx5guyb3D5k16oD+PeIeBwgIp6QdD9ZR5Zm+R1wQjo9vpC1H7lbz0yyU4Qnkj5oU2yvSDoa+H460FeSfZuv9WeyDiajgK9HxLL0k6DfSGon67z1QFrm05JuST99+i0wPbecHwLnplOVnWSd2Valz8NCIuJOSb9I63yErPNS3pbAryV1tRa/lMq/CPxU0r+mbXkuLW+Jstslzyfr2HVXD8tZa19GxEOFg++Fmv24kr78RcYAAAMLSURBVCypd017VdmtoP9E1q/jgTRpSNrGoSnm70TEs5J+A1wuaQLw+Yio3WfN8HbgbElrgFeBfyH74LyH7IN3bq7up4AZkl4iSwzdiojlko4Hfp6+0EB2OvQF6r9GZ0sak8quI3tf9oevA98F5qckv5jsy9cPgSskfZTsEteAttpr9PazpNH76BpJbwNuS+/nF4FjyVr/Zah3vG0GXCDp/5G9N7pzM9nZ1Z2BSyKiPZ1tKLy+9Pl5FHBOes9tRPb6N6shV5h/JleC1KK9MyIaXS9G0uZkH3J7FLhONOhoEP/EqjfS67AyIkLSRLKOQhMGOi6z9UkV3kfpy2JbREwe6FiaxS34JkunbG4gO13TqM5BZD+b+fb6mNwrZk/gB6ll9SxN6g9htoHx+2gQcgvezMysgtzJzszMrIKc4M3MzCrICd7MzKyCnODNNkCSVkual/ub0oRltkr6eG58nZ8+aGZ95052ZhsgSS9GxBZNXub+ZPe2/2Azl2tm68YteDP7G0mLJX1D0m2S2iXtIWmOpIcknZDqSNLZku6VdE+6KRJkT3DbN50R+JKy581flebZVtKVkuZLul3SP6XyqZJmSLpB0qJ0oyAzawL/Dt5sw7SZsgdhdPlmRPwiDS+JiH0kfYfslsrvJruH9wKyx6B+mOz2r+8ge6LWXEk3AVPIteBTi77L14C7IuJISQcCP+Hvt/l9K3AA2V0CF0r674h4tdkbbLahcYI32zCtjIjdGkzrehLfPWTPV38BeEHSy5K2JveUROAJSTcCe5E9xa2R95DuAx4R1yu7X/7QNO3qiFgFrJL0JNnTB5f2aevMzKfozew1up7ctoa1n+K2hr8/xa236s3T1QGoN08MNLOCnODNrLcaPSWx6BMD9weeiojuWvxm1kf+pmy2Yaq9Bv+7iCj6U7m6T0mU9DTQKelusmv3d+XmmQpcmJ5O9hJwXB/jN7Me+GdyZmZmFeRT9GZmZhXkBG9mZlZBTvBmZmYV5ARvZmZWQU7wZmZmFeQEb2ZmVkFO8GZmZhX0/wG91UKA04WQEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check class distribution\n",
    "distribution = {}\n",
    "for emo in train_label:\n",
    "    if emo in distribution:\n",
    "        distribution[emo] += 1\n",
    "    else:\n",
    "        distribution[emo] = 1\n",
    "emo_lst = [key for key in distribution.keys()]\n",
    "num_lst = np.array([distribution[emo] for emo in emo_lst])\n",
    "num_lst = num_lst / sum(num_lst)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.bar(emo_lst, num_lst)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out that it's an unbalanced dataset. (8 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# encode label\n",
    "from keras import utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_label)\n",
    "train_label_encoded = label_encoder.transform(train_label)\n",
    "# label encode (one hot)\n",
    "y_train = utils.to_categorical(train_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'anticipation', 'joy', ..., 'surprise', 'disgust',\n",
       "       'sadness'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 4, ..., 6, 2, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Encode the label from string to numbers (Ex: 'joy' -> 4).  \n",
    "2. Then, turn into one hot for classification (Ex: 4 -> 0 0 0 0 1 0 0 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for text preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(sentence, save_stop_words=False):\n",
    "    # remove punctuation\n",
    "    print('remove punctuation')\n",
    "    sentence = sentence.apply(lambda x: remove_punctuation(x))\n",
    "    # tokenizer\n",
    "    print('tokenize')\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    sentence = sentence.apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    # remove stop words\n",
    "    if not save_stop_words:\n",
    "        print('remove stop words')\n",
    "        sentence = sentence.apply(lambda x: remove_stopwords(x))\n",
    "    # lemmatizer\n",
    "    print('lemmatize')\n",
    "    sentence = sentence.apply(lambda x: word_lemmatizer(x))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing the text plays an important role in NLP.**    \n",
    "  * Procedure: remove punctuation -> tokenize -> (remove stop words) -> lemmatize \n",
    "  * Since stop words might be useful in lstm model, I run two different preprocess procedure.(saving stop words or not) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation\n",
      "tokenize\n",
      "remove stop words\n",
      "lemmatize\n",
      "remove punctuation\n",
      "tokenize\n",
      "remove stop words\n",
      "lemmatize\n"
     ]
    }
   ],
   "source": [
    "# clean test data\n",
    "train_data_df['text_clean'] = preprocess(train_data_df['text'], save_stop_words=False)\n",
    "test_data_df['text_clean'] = preprocess(test_data_df['text'], save_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove punctuation\n",
      "tokenize\n",
      "lemmatize\n",
      "remove punctuation\n",
      "tokenize\n",
      "lemmatize\n"
     ]
    }
   ],
   "source": [
    "# clean test data (save stop words)\n",
    "train_data_df['text_clean_ws'] = preprocess(train_data_df['text'], save_stop_words=True)\n",
    "test_data_df['text_clean_ws'] = preprocess(test_data_df['text'], save_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.455563e+06\n",
       "mean     1.487073e+01\n",
       "std      6.299944e+00\n",
       "min      1.000000e+00\n",
       "25%      1.000000e+01\n",
       "50%      1.500000e+01\n",
       "75%      2.000000e+01\n",
       "max      4.300000e+01\n",
       "Name: sentence_len, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence length analysis\n",
    "train_data_df['sentence_len'] = train_data_df['text_clean_ws'].apply(lambda x: len(x))\n",
    "train_data_df['sentence_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    411972.000000\n",
       "mean         17.800661\n",
       "std           5.289889\n",
       "min           3.000000\n",
       "25%          14.000000\n",
       "50%          18.000000\n",
       "75%          22.000000\n",
       "max          59.000000\n",
       "Name: sentence_len, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df['sentence_len'] = test_data_df['text_clean_ws'].apply(lambda x: len(x))\n",
    "test_data_df['sentence_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed text data\n",
    "train_data_df.to_pickle('./data/train.pkl')\n",
    "test_data_df.to_pickle('./data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_ws</th>\n",
       "      <th>sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f10</td>\n",
       "      <td>o m g Shut Up And Dance though #BlackMirror &lt;LH&gt;</td>\n",
       "      <td>[g, shut, dance, though, blackmirror, lh]</td>\n",
       "      <td>[o, m, g, shut, up, and, dance, though, blackm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f11</td>\n",
       "      <td>On #twitch &lt;LH&gt; on the #Destinybeta #Destiny #...</td>\n",
       "      <td>[twitch, lh, destinybeta, destiny, destiny2, d...</td>\n",
       "      <td>[on, twitch, lh, on, the, destinybeta, destiny...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f14</td>\n",
       "      <td>A nice sunny wak this morning not many &lt;LH&gt; ar...</td>\n",
       "      <td>[nice, sunny, wak, morning, many, lh, aroud, w...</td>\n",
       "      <td>[a, nice, sunny, wak, this, morning, not, many...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f15</td>\n",
       "      <td>I'm one of those people who love candy corn......</td>\n",
       "      <td>[im, one, people, love, candy, corn, lot, üòÅüòÇ, ...</td>\n",
       "      <td>[im, one, of, those, people, who, love, candy,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f16</td>\n",
       "      <td>@metmuseum What are these? They look like some...</td>\n",
       "      <td>[metmuseum, look, like, something, toddler, ma...</td>\n",
       "      <td>[metmuseum, what, are, these, they, look, like...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x1c7f10   o m g Shut Up And Dance though #BlackMirror <LH>   \n",
       "1  0x1c7f11  On #twitch <LH> on the #Destinybeta #Destiny #...   \n",
       "2  0x1c7f14  A nice sunny wak this morning not many <LH> ar...   \n",
       "3  0x1c7f15  I'm one of those people who love candy corn......   \n",
       "4  0x1c7f16  @metmuseum What are these? They look like some...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0          [g, shut, dance, though, blackmirror, lh]   \n",
       "1  [twitch, lh, destinybeta, destiny, destiny2, d...   \n",
       "2  [nice, sunny, wak, morning, many, lh, aroud, w...   \n",
       "3  [im, one, people, love, candy, corn, lot, üòÅüòÇ, ...   \n",
       "4  [metmuseum, look, like, something, toddler, ma...   \n",
       "\n",
       "                                       text_clean_ws  sentence_len  \n",
       "0  [o, m, g, shut, up, and, dance, though, blackm...            10  \n",
       "1  [on, twitch, lh, on, the, destinybeta, destiny...            16  \n",
       "2  [a, nice, sunny, wak, this, morning, not, many...            24  \n",
       "3  [im, one, of, those, people, who, love, candy,...            18  \n",
       "4  [metmuseum, what, are, these, they, look, like...            17  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_ws</th>\n",
       "      <th>sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x1c7f0f</td>\n",
       "      <td>@JZED74 While inappropriate AF, he likely wasn...</td>\n",
       "      <td>[jzed74, inappropriate, af, likely, wasnt, kid...</td>\n",
       "      <td>[jzed74, while, inappropriate, af, he, likely,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x1c7f12</td>\n",
       "      <td>I tried to figure out why you mean so much to ...</td>\n",
       "      <td>[tried, figure, mean, much, couldnt, think, si...</td>\n",
       "      <td>[i, tried, to, figure, out, why, you, mean, so...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1c7f13</td>\n",
       "      <td>The only ‚Äúbig plan‚Äù you ever had in your life,...</td>\n",
       "      <td>[‚Äúbig, plan, ‚Äù, ever, life, promote, turnbullm...</td>\n",
       "      <td>[the, only, ‚Äúbig, plan, ‚Äù, you, ever, had, in,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1c7f17</td>\n",
       "      <td>Looking back on situations old &amp; new, recent o...</td>\n",
       "      <td>[looking, back, situation, old, new, recent, w...</td>\n",
       "      <td>[looking, back, on, situation, old, new, recen...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1c7f18</td>\n",
       "      <td>@jasoninthehouse Why do you insist on talking ...</td>\n",
       "      <td>[jasoninthehouse, insist, talking, clinton, wh...</td>\n",
       "      <td>[jasoninthehouse, why, do, you, insist, on, ta...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x1c7f0f  @JZED74 While inappropriate AF, he likely wasn...   \n",
       "1  0x1c7f12  I tried to figure out why you mean so much to ...   \n",
       "2  0x1c7f13  The only ‚Äúbig plan‚Äù you ever had in your life,...   \n",
       "3  0x1c7f17  Looking back on situations old & new, recent o...   \n",
       "4  0x1c7f18  @jasoninthehouse Why do you insist on talking ...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [jzed74, inappropriate, af, likely, wasnt, kid...   \n",
       "1  [tried, figure, mean, much, couldnt, think, si...   \n",
       "2  [‚Äúbig, plan, ‚Äù, ever, life, promote, turnbullm...   \n",
       "3  [looking, back, situation, old, new, recent, w...   \n",
       "4  [jasoninthehouse, insist, talking, clinton, wh...   \n",
       "\n",
       "                                       text_clean_ws  sentence_len  \n",
       "0  [jzed74, while, inappropriate, af, he, likely,...             9  \n",
       "1  [i, tried, to, figure, out, why, you, mean, so...            28  \n",
       "2  [the, only, ‚Äúbig, plan, ‚Äù, you, ever, had, in,...            19  \n",
       "3  [looking, back, on, situation, old, new, recen...            22  \n",
       "4  [jasoninthehouse, why, do, you, insist, on, ta...            24  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# load preprocessed text data\n",
    "train_data_df = pd.read_pickle('./data/train.pkl')\n",
    "test_data_df = pd.read_pickle('./data/test.pkl')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def get_seqs(text, tokenizer, max_length=128):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "def train_tokenizer(X, max_num_words=20000):\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    return tokenizer\n",
    "\n",
    "def build_weight(y):\n",
    "    y_int = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_int), y_int)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "    return d_class_weights\n",
    "\n",
    "def train_model(model, X, y, setup):\n",
    "    class_weight = build_weight(y)\n",
    "    callbacks = [ModelCheckpoint(filepath='model_{}.h5'.format(setup['type']), \n",
    "                                 monitor='val_accuracy', \n",
    "                                 save_best_only=True)]\n",
    "    optimizer = optimizers.Adam(lr=setup['lr'], clipvalue=1.0)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(X, y, epochs=setup['epochs'], batch_size=setup['batch_size'], callbacks=callbacks,\n",
    "              validation_split=0.1, class_weight=class_weight)\n",
    "    return model\n",
    "\n",
    "def show_result(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('mean f1 score: %.4f' % f1)\n",
    "    print('confusion matrix:\\n{}'.format(cm))\n",
    "    return f1\n",
    "\n",
    "def make_prediction(model, X_train, y_train, X_test, setup):\n",
    "    # train model\n",
    "    model = train_model(model, X_train, y_train, setup)\n",
    "    # predict\n",
    "    model.load_weights('model_{}.h5'.format(setup['type']))\n",
    "    probs = model.predict(X_test, verbose=1)\n",
    "    preds = np.argmax(probs, axis=1).reshape(-1,)\n",
    "    # record\n",
    "    return preds\n",
    "\n",
    "def make_submission(tweet_id_lst, emotion_lst, model_type):\n",
    "    assert len(tweet_id_lst) == len(emotion_lst)\n",
    "    prediction_dict = {}\n",
    "    for idx, tweet_id in enumerate(tweet_id_lst):\n",
    "        prediction_dict[tweet_id] = emotion_lst[idx]\n",
    "    # submission\n",
    "    submission_df = pd.read_csv('./sampleSubmission.csv')\n",
    "    # write\n",
    "    assert len(tweet_id_lst) == len(submission_df['id'])\n",
    "    for idx, tweet_id in enumerate(submission_df['id']):\n",
    "        submission_df.iloc[idx]['emotion'] = prediction_dict[tweet_id]\n",
    "    submission_df.to_csv(\"./submission_{}.csv\".format(model_type), index=False)\n",
    "    print('submission file is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. During the training step, I set 0.1 as validation set to monitor if the model overfits. Pick the one with best validation performance.  \n",
    "2. Since the dataset is imbalanced, class weight is set to pay more attention (additional loss) on the misclassification in smaller class set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation to evaluate the method\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_evaluate(X, y, setup, cv=5):\n",
    "    # train\n",
    "    kf = KFold(n_splits=cv)\n",
    "    y_pred_lst, y_true_lst = [], []\n",
    "    for idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print('Fold - {}'.format(idx+1))\n",
    "        # split\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        # build model\n",
    "        if setup['type'] == 'dnn':\n",
    "            model = dnn(setup)\n",
    "        elif setup['type'] == 'blstm_att':\n",
    "            model = blstm_att(setup)\n",
    "        else:\n",
    "            raise Exception('Undefined structure')\n",
    "        # train model & make prediction\n",
    "        preds = make_prediction(model, X_train, y_train, X_val, setup)\n",
    "        # record\n",
    "        y_true = np.argmax(y_val, axis=1).reshape(-1,)\n",
    "        y_true_lst.extend(y_true)\n",
    "        y_pred_lst.extend(preds)\n",
    "    # evaluate\n",
    "    show_result(y_true_lst, y_pred_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the best model with best parameters setting, cross validation is a great method to evaluate the performance of different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1. \n",
    "tfidf + dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "def dnn(setup):\n",
    "    # parameter setting\n",
    "    input_dim = setup['max_features']\n",
    "    p = setup['dropout']\n",
    "    weight_decay = setup['weight_decay']\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay),\n",
    "                    input_shape=(input_dim,)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    model.add(Dense(64, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    model.add(Dense(8, activation='softmax',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/borris/Toolkits/anaconda3/envs/env_tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/10\n",
      "1048005/1048005 [==============================] - 222s 212us/step - loss: 1.8331 - accuracy: 0.4092 - val_loss: 1.7676 - val_accuracy: 0.4461\n",
      "Epoch 2/10\n",
      "1048005/1048005 [==============================] - 220s 210us/step - loss: 1.8255 - accuracy: 0.4345 - val_loss: 1.7693 - val_accuracy: 0.4490\n",
      "Epoch 3/10\n",
      "1048005/1048005 [==============================] - 223s 212us/step - loss: 1.8215 - accuracy: 0.4397 - val_loss: 1.7610 - val_accuracy: 0.4601\n",
      "Epoch 4/10\n",
      "1048005/1048005 [==============================] - 223s 213us/step - loss: 1.8164 - accuracy: 0.4424 - val_loss: 1.7620 - val_accuracy: 0.4606\n",
      "Epoch 5/10\n",
      "1048005/1048005 [==============================] - 223s 213us/step - loss: 1.8132 - accuracy: 0.4427 - val_loss: 1.7593 - val_accuracy: 0.4562\n",
      "Epoch 6/10\n",
      "1048005/1048005 [==============================] - 224s 213us/step - loss: 1.8113 - accuracy: 0.4433 - val_loss: 1.7567 - val_accuracy: 0.4594\n",
      "Epoch 7/10\n",
      "1048005/1048005 [==============================] - 225s 215us/step - loss: 1.8075 - accuracy: 0.4429 - val_loss: 1.7566 - val_accuracy: 0.4570\n",
      "Epoch 8/10\n",
      "1048005/1048005 [==============================] - 225s 215us/step - loss: 1.8065 - accuracy: 0.4442 - val_loss: 1.7609 - val_accuracy: 0.4534\n",
      "Epoch 9/10\n",
      "1048005/1048005 [==============================] - 224s 214us/step - loss: 1.8044 - accuracy: 0.4443 - val_loss: 1.7574 - val_accuracy: 0.4516\n",
      "Epoch 10/10\n",
      "1048005/1048005 [==============================] - 223s 213us/step - loss: 1.8032 - accuracy: 0.4442 - val_loss: 1.7543 - val_accuracy: 0.4507\n",
      "291113/291113 [==============================] - 40s 138us/step\n",
      "Fold - 2\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/10\n",
      "1048005/1048005 [==============================] - 222s 212us/step - loss: 1.8300 - accuracy: 0.4126 - val_loss: 1.7705 - val_accuracy: 0.4480\n",
      "Epoch 2/10\n",
      "1048005/1048005 [==============================] - 223s 212us/step - loss: 1.8227 - accuracy: 0.4397 - val_loss: 1.7669 - val_accuracy: 0.4625\n",
      "Epoch 3/10\n",
      "1048005/1048005 [==============================] - 224s 214us/step - loss: 1.8185 - accuracy: 0.4435 - val_loss: 1.7633 - val_accuracy: 0.4543\n",
      "Epoch 4/10\n",
      "1048005/1048005 [==============================] - 220s 210us/step - loss: 1.8127 - accuracy: 0.4462 - val_loss: 1.7604 - val_accuracy: 0.4561\n",
      "Epoch 5/10\n",
      "1048005/1048005 [==============================] - 221s 211us/step - loss: 1.8095 - accuracy: 0.4455 - val_loss: 1.7612 - val_accuracy: 0.4678\n",
      "Epoch 6/10\n",
      "1048005/1048005 [==============================] - 216s 206us/step - loss: 1.8074 - accuracy: 0.4452 - val_loss: 1.7584 - val_accuracy: 0.4616\n",
      "Epoch 7/10\n",
      "1048005/1048005 [==============================] - 221s 211us/step - loss: 1.8047 - accuracy: 0.4456 - val_loss: 1.7565 - val_accuracy: 0.4558\n",
      "Epoch 8/10\n",
      "1048005/1048005 [==============================] - 219s 209us/step - loss: 1.8021 - accuracy: 0.4460 - val_loss: 1.7555 - val_accuracy: 0.4654\n",
      "Epoch 9/10\n",
      "1048005/1048005 [==============================] - 219s 209us/step - loss: 1.8013 - accuracy: 0.4452 - val_loss: 1.7541 - val_accuracy: 0.4590\n",
      "Epoch 10/10\n",
      "1048005/1048005 [==============================] - 219s 209us/step - loss: 1.8000 - accuracy: 0.4449 - val_loss: 1.7520 - val_accuracy: 0.4516\n",
      "291113/291113 [==============================] - 40s 138us/step\n",
      "Fold - 3\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/10\n",
      "1048005/1048005 [==============================] - 218s 208us/step - loss: 1.8324 - accuracy: 0.4124 - val_loss: 1.7708 - val_accuracy: 0.4585\n",
      "Epoch 2/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8280 - accuracy: 0.4373 - val_loss: 1.7702 - val_accuracy: 0.4592\n",
      "Epoch 3/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8235 - accuracy: 0.4408 - val_loss: 1.7687 - val_accuracy: 0.4526\n",
      "Epoch 4/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8207 - accuracy: 0.4424 - val_loss: 1.7639 - val_accuracy: 0.4567\n",
      "Epoch 5/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8177 - accuracy: 0.4419 - val_loss: 1.7675 - val_accuracy: 0.4555\n",
      "Epoch 6/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8149 - accuracy: 0.4426 - val_loss: 1.7652 - val_accuracy: 0.4578\n",
      "Epoch 7/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8121 - accuracy: 0.4425 - val_loss: 1.7614 - val_accuracy: 0.4623\n",
      "Epoch 8/10\n",
      "1048005/1048005 [==============================] - 216s 207us/step - loss: 1.8086 - accuracy: 0.4430 - val_loss: 1.7581 - val_accuracy: 0.4667\n",
      "Epoch 9/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8069 - accuracy: 0.4431 - val_loss: 1.7620 - val_accuracy: 0.4383\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048005/1048005 [==============================] - 216s 206us/step - loss: 1.8056 - accuracy: 0.4441 - val_loss: 1.7579 - val_accuracy: 0.4579\n",
      "291113/291113 [==============================] - 41s 140us/step\n",
      "Fold - 4\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116446 samples\n",
      "Epoch 1/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8317 - accuracy: 0.4102 - val_loss: 1.7696 - val_accuracy: 0.4510\n",
      "Epoch 2/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8265 - accuracy: 0.4362 - val_loss: 1.7663 - val_accuracy: 0.4487\n",
      "Epoch 3/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8222 - accuracy: 0.4402 - val_loss: 1.7630 - val_accuracy: 0.4559\n",
      "Epoch 4/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8176 - accuracy: 0.4422 - val_loss: 1.7581 - val_accuracy: 0.4442\n",
      "Epoch 5/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8141 - accuracy: 0.4427 - val_loss: 1.7635 - val_accuracy: 0.4522\n",
      "Epoch 6/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8126 - accuracy: 0.4418 - val_loss: 1.7615 - val_accuracy: 0.4605\n",
      "Epoch 7/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8104 - accuracy: 0.4424 - val_loss: 1.7589 - val_accuracy: 0.4589\n",
      "Epoch 8/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8079 - accuracy: 0.4426 - val_loss: 1.7599 - val_accuracy: 0.4614\n",
      "Epoch 9/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8062 - accuracy: 0.4432 - val_loss: 1.7605 - val_accuracy: 0.4670\n",
      "Epoch 10/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8051 - accuracy: 0.4433 - val_loss: 1.7586 - val_accuracy: 0.4520\n",
      "291112/291112 [==============================] - 42s 143us/step\n",
      "Fold - 5\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116446 samples\n",
      "Epoch 1/10\n",
      "1048005/1048005 [==============================] - 218s 208us/step - loss: 1.8306 - accuracy: 0.4099 - val_loss: 1.7713 - val_accuracy: 0.4537\n",
      "Epoch 2/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8229 - accuracy: 0.4369 - val_loss: 1.7747 - val_accuracy: 0.4659\n",
      "Epoch 3/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8193 - accuracy: 0.4410 - val_loss: 1.7675 - val_accuracy: 0.4630\n",
      "Epoch 4/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8148 - accuracy: 0.4420 - val_loss: 1.7652 - val_accuracy: 0.4515\n",
      "Epoch 5/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8120 - accuracy: 0.4418 - val_loss: 1.7644 - val_accuracy: 0.4647\n",
      "Epoch 6/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8089 - accuracy: 0.4429 - val_loss: 1.7647 - val_accuracy: 0.4665\n",
      "Epoch 7/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8064 - accuracy: 0.4427 - val_loss: 1.7647 - val_accuracy: 0.4587\n",
      "Epoch 8/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8048 - accuracy: 0.4439 - val_loss: 1.7628 - val_accuracy: 0.4653\n",
      "Epoch 9/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8028 - accuracy: 0.4435 - val_loss: 1.7643 - val_accuracy: 0.4563\n",
      "Epoch 10/10\n",
      "1048005/1048005 [==============================] - 217s 207us/step - loss: 1.8028 - accuracy: 0.4442 - val_loss: 1.7624 - val_accuracy: 0.4581\n",
      "291112/291112 [==============================] - 43s 146us/step\n",
      "mean f1 score: 0.4094\n",
      "confusion matrix:\n",
      "[[ 20623   1744   6260   2220   2363   2856   2124   1677]\n",
      " [ 14389 139238  13650  13624  25996   8950   9937  23151]\n",
      " [ 21193   5359  63942   7727   7692  16680  10610   5898]\n",
      " [  4890   3974   4911  35279   5138   3038   3650   3119]\n",
      " [ 33627  54163  31985  31590 237629  21734  28230  77059]\n",
      " [ 26230   9279  37876  11497  14421  71395  13886   8853]\n",
      " [  4734   2778   7684   3475   4632   4214  18340   2872]\n",
      " [ 11983  24060  13876  10322  36843   7709   9253  91432]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate by cross validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# parameter setup\n",
    "setup = {'max_features': 40000, 'dropout': 0.5, 'weight_decay': 1e-4, \n",
    "         'batch_size': 256, 'lr': 0.001, 'epochs': 10, 'type': 'dnn'}\n",
    "\n",
    "# train vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=setup['max_features'])\n",
    "vectorizer.fit(train_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# feature encode\n",
    "X_train = vectorizer.transform(train_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# cross validation\n",
    "cv_evaluate(X_train, y_train, setup, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               10240256  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 10,257,224\n",
      "Trainable params: 10,257,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1310006 samples, validate on 145557 samples\n",
      "Epoch 1/10\n",
      "1310006/1310006 [==============================] - 273s 208us/step - loss: 1.8263 - accuracy: 0.4168 - val_loss: 1.7629 - val_accuracy: 0.4673\n",
      "Epoch 2/10\n",
      "1310006/1310006 [==============================] - 273s 208us/step - loss: 1.8229 - accuracy: 0.4375 - val_loss: 1.7605 - val_accuracy: 0.4511\n",
      "Epoch 3/10\n",
      "1310006/1310006 [==============================] - 273s 209us/step - loss: 1.8181 - accuracy: 0.4406 - val_loss: 1.7607 - val_accuracy: 0.4520\n",
      "Epoch 4/10\n",
      "1310006/1310006 [==============================] - 273s 208us/step - loss: 1.8137 - accuracy: 0.4408 - val_loss: 1.7589 - val_accuracy: 0.4630\n",
      "Epoch 5/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8116 - accuracy: 0.4413 - val_loss: 1.7517 - val_accuracy: 0.4549\n",
      "Epoch 6/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8102 - accuracy: 0.4403 - val_loss: 1.7560 - val_accuracy: 0.4558\n",
      "Epoch 7/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8093 - accuracy: 0.4409 - val_loss: 1.7518 - val_accuracy: 0.4627\n",
      "Epoch 8/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8065 - accuracy: 0.4415 - val_loss: 1.7517 - val_accuracy: 0.4570\n",
      "Epoch 9/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8061 - accuracy: 0.4415 - val_loss: 1.7502 - val_accuracy: 0.4623\n",
      "Epoch 10/10\n",
      "1310006/1310006 [==============================] - 274s 209us/step - loss: 1.8047 - accuracy: 0.4417 - val_loss: 1.7487 - val_accuracy: 0.4540\n",
      "411972/411972 [==============================] - 60s 146us/step\n",
      "submission file is done\n"
     ]
    }
   ],
   "source": [
    "# train on whole dataset and make submission\n",
    "\n",
    "# feature encode\n",
    "X_test = vectorizer.transform(test_data_df['text_clean'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# train & predict\n",
    "model = dnn(setup)\n",
    "y_pred = make_prediction(model, X_train, y_train, X_test, setup)\n",
    "\n",
    "# make submission\n",
    "emotion_lst = label_encoder.inverse_transform(y_pred)\n",
    "make_submission(test_data_df['tweet_id'], emotion_lst, setup['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "**tfidf with simple dnn is a simple method. I take it as a baseline.**\n",
    "1. dropout is added to avoid overfitting\n",
    "2. I didn't take ML model as my baseline model since the dataset is large. when I ran svm model, it took a really long time and my computer finally crashed  \n",
    "\n",
    "**result:**  \n",
    "mean f1 score: **0.4094** (cross validation: fold=5)  \n",
    "compare to the submission score: **0.3619** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2. \n",
    "word2vec + blstm + self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention layer\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, att_head, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.att_head = att_head\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.vec_dim = int(self.output_dim*self.att_head)\n",
    "        self.pro_kernel = self.add_weight(name='kernel_p',\n",
    "                                        shape=(3, input_shape[2], self.vec_dim),\n",
    "                                        initializer='orthogonal',\n",
    "                                        trainable=True)\n",
    "        self.out_kernel = self.add_weight(name='kernal_o',\n",
    "                                        shape=(1, self.vec_dim, input_shape[2]),\n",
    "                                        initializer='orthogonal',\n",
    "                                        trainable=True)\n",
    "        super(Self_Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # projection\n",
    "        WQ = K.dot(x, self.pro_kernel[0])\n",
    "        WK = K.dot(x, self.pro_kernel[1])\n",
    "        WV = K.dot(x, self.pro_kernel[2])\n",
    "        # attention score\n",
    "        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))\n",
    "        # scale\n",
    "        QK = QK / (self.vec_dim**0.5)\n",
    "        # softmax normalization\n",
    "        QK = K.softmax(QK)\n",
    "        # weighted vector \n",
    "        V = K.batch_dot(QK, WV)\n",
    "        # project to output space\n",
    "        Z = K.dot(V, self.out_kernel[0])\n",
    "        return Z\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "\n",
    "# define structure\n",
    "def blstm_att(setup):\n",
    "    # parameter setting\n",
    "    max_num_words = setup['max_num_words']\n",
    "    max_length = setup['max_length']\n",
    "    att_head = setup['att_head']\n",
    "    p = setup['dropout']\n",
    "    weight_decay = setup['weight_decay']\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # word embedding\n",
    "    model.add(Embedding(max_num_words, 128, input_length=max_length))\n",
    "    # blstm\n",
    "    model.add(Bidirectional(LSTM(units=128, \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=p, \n",
    "                                 recurrent_dropout=p, \n",
    "                                 kernel_initializer='orthogonal',\n",
    "                                 kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                                 bias_regularizer=keras.regularizers.l2(weight_decay)\n",
    "                                 )))\n",
    "    # self attention\n",
    "    model.add(Self_Attention(128, att_head))\n",
    "    # average pooling\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=p))\n",
    "    # dimension reduction\n",
    "    model.add(Dense(256, activation='relu',\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    model.add(Dropout(rate=p))\n",
    "    # classifier\n",
    "    model.add(Dense(8, activation='softmax',\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer='orthogonal',\n",
    "                    kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                    bias_regularizer=regularizers.l2(weight_decay)\n",
    "                    ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 990725 unique tokens.\n",
      "Fold - 1\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 128)           12800000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64, 256)           263168    \n",
      "_________________________________________________________________\n",
      "self__attention_1 (Self_Atte (None, 64, 256)           524288    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 8)                 2048      \n",
      "=================================================================\n",
      "Total params: 13,655,296\n",
      "Trainable params: 13,655,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/5\n",
      "1048005/1048005 [==============================] - 644s 615us/step - loss: 1.6146 - accuracy: 0.4072 - val_loss: 1.4532 - val_accuracy: 0.4910\n",
      "Epoch 2/5\n",
      "1048005/1048005 [==============================] - 660s 630us/step - loss: 1.4204 - accuracy: 0.4944 - val_loss: 1.4182 - val_accuracy: 0.4879\n",
      "Epoch 3/5\n",
      "1048005/1048005 [==============================] - 634s 605us/step - loss: 1.3443 - accuracy: 0.5199 - val_loss: 1.4125 - val_accuracy: 0.5009\n",
      "Epoch 4/5\n",
      "1048005/1048005 [==============================] - 719s 686us/step - loss: 1.2923 - accuracy: 0.5361 - val_loss: 1.4121 - val_accuracy: 0.4986\n",
      "Epoch 5/5\n",
      "1048005/1048005 [==============================] - 707s 675us/step - loss: 1.2569 - accuracy: 0.5472 - val_loss: 1.4502 - val_accuracy: 0.5154\n",
      "291113/291113 [==============================] - 278s 955us/step\n",
      "Fold - 2\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 64, 128)           12800000  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64, 256)           263168    \n",
      "_________________________________________________________________\n",
      "self__attention_2 (Self_Atte (None, 64, 256)           524288    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 2048      \n",
      "=================================================================\n",
      "Total params: 13,655,296\n",
      "Trainable params: 13,655,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/5\n",
      "1048005/1048005 [==============================] - 730s 697us/step - loss: 1.6322 - accuracy: 0.3941 - val_loss: 1.4777 - val_accuracy: 0.4668\n",
      "Epoch 2/5\n",
      "1048005/1048005 [==============================] - 706s 674us/step - loss: 1.4480 - accuracy: 0.4799 - val_loss: 1.4267 - val_accuracy: 0.5026\n",
      "Epoch 3/5\n",
      "1048005/1048005 [==============================] - 700s 668us/step - loss: 1.3744 - accuracy: 0.5064 - val_loss: 1.4104 - val_accuracy: 0.4902\n",
      "Epoch 4/5\n",
      "1048005/1048005 [==============================] - 704s 672us/step - loss: 1.3227 - accuracy: 0.5251 - val_loss: 1.4180 - val_accuracy: 0.5017\n",
      "Epoch 5/5\n",
      "1048005/1048005 [==============================] - 659s 629us/step - loss: 1.2814 - accuracy: 0.5381 - val_loss: 1.4199 - val_accuracy: 0.5083\n",
      "291113/291113 [==============================] - 266s 914us/step\n",
      "Fold - 3\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 64, 128)           12800000  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 64, 256)           263168    \n",
      "_________________________________________________________________\n",
      "self__attention_3 (Self_Atte (None, 64, 256)           524288    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 2048      \n",
      "=================================================================\n",
      "Total params: 13,655,296\n",
      "Trainable params: 13,655,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1048005 samples, validate on 116445 samples\n",
      "Epoch 1/5\n",
      "1048005/1048005 [==============================] - 708s 676us/step - loss: 1.6121 - accuracy: 0.4055 - val_loss: 1.4632 - val_accuracy: 0.4696\n",
      "Epoch 2/5\n",
      "1048005/1048005 [==============================] - 674s 643us/step - loss: 1.4172 - accuracy: 0.4929 - val_loss: 1.4070 - val_accuracy: 0.4747\n",
      "Epoch 3/5\n",
      "1048005/1048005 [==============================] - 774s 738us/step - loss: 1.3381 - accuracy: 0.5198 - val_loss: 1.4243 - val_accuracy: 0.5070\n",
      "Epoch 4/5\n",
      "1048005/1048005 [==============================] - 771s 736us/step - loss: 1.2842 - accuracy: 0.5376 - val_loss: 1.4181 - val_accuracy: 0.4976\n",
      "Epoch 5/5\n",
      "1048005/1048005 [==============================] - 763s 728us/step - loss: 1.2457 - accuracy: 0.5489 - val_loss: 1.4493 - val_accuracy: 0.5042\n",
      "291113/291113 [==============================] - 273s 939us/step\n",
      "Fold - 4\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 64, 128)           12800000  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64, 256)           263168    \n",
      "_________________________________________________________________\n",
      "self__attention_4 (Self_Atte (None, 64, 256)           524288    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 8)                 2048      \n",
      "=================================================================\n",
      "Total params: 13,655,296\n",
      "Trainable params: 13,655,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1048005 samples, validate on 116446 samples\n",
      "Epoch 1/5\n",
      "1048005/1048005 [==============================] - 687s 655us/step - loss: 1.6363 - accuracy: 0.3998 - val_loss: 1.4761 - val_accuracy: 0.4620\n",
      "Epoch 2/5\n",
      "1048005/1048005 [==============================] - 650s 620us/step - loss: 1.4495 - accuracy: 0.4814 - val_loss: 1.4234 - val_accuracy: 0.4925\n",
      "Epoch 3/5\n",
      "1048005/1048005 [==============================] - 666s 635us/step - loss: 1.3729 - accuracy: 0.5079 - val_loss: 1.4086 - val_accuracy: 0.5169\n",
      "Epoch 4/5\n",
      "1048005/1048005 [==============================] - 664s 634us/step - loss: 1.3210 - accuracy: 0.5257 - val_loss: 1.4090 - val_accuracy: 0.5016\n",
      "Epoch 5/5\n",
      " 876160/1048005 [========================>.....] - ETA: 1:44 - loss: 1.2726 - accuracy: 0.5401"
     ]
    }
   ],
   "source": [
    "# evaluate by cross validation\n",
    "\n",
    "# parameter setup\n",
    "setup = {'max_num_words': 100000, 'max_length': 64, 'att_head': 4, 'dropout': 0.5, 'weight_decay': 1e-4,\n",
    "         'batch_size': 128, 'lr': 0.001, 'epochs': 5, 'type': 'blstm_att'}\n",
    "\n",
    "# train tokenizer\n",
    "tokenizer = train_tokenizer(train_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), setup['max_num_words'])\n",
    "\n",
    "# feature encode\n",
    "X_train = get_seqs(train_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), tokenizer, setup['max_length'])\n",
    "\n",
    "# cross validation\n",
    "cv_evaluate(X_train, y_train, setup, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on whole dataset and make submission\n",
    "\n",
    "# feature encode\n",
    "X_test = get_seqs(test_data_df['text_clean_ws'].apply(lambda x: ' '.join(x)), tokenizer, setup['max_length'])\n",
    "\n",
    "# train & predict\n",
    "model = blstm_att(setup)\n",
    "y_pred = make_prediction(model, X_train, y_train, X_test, setup)\n",
    "\n",
    "# make submission\n",
    "emotion_lst = label_encoder.inverse_transform(y_pred)\n",
    "make_submission(test_data_df['tweet_id'], emotion_lst, setup['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "**STEP:**\n",
    "1. First of all, for a network to understand the words, we first turn the words into word embedding.  \n",
    "2. Since the words in text are in sequence, LSTM is a good choice, while bidirectional version helps better capture the information in an sentence.  \n",
    "3. Nowadays, attention method is quite popular to focus on the more important words for semantic understanding. Therefore, I implement a self attention function, based on \"transformer\", to encode words in the sentence.  \n",
    "4. By AveragePooling, average the embeddings in all words as a sentence embedding.   \n",
    "5. With Dense Layers, classify the category of the sentences\n",
    "6. Dropout is added to avoid overfitting\n",
    "\n",
    "**RESULT:**  \n",
    "mean f1 score:  (cross validation: fold=5)  \n",
    "compare to the submission score: **0.48428**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
